{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshaychitkara/2210990536/blob/main/Research-paper-RAGed-Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpn9SvsikIX_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17IDh3BokRGi"
      },
      "source": [
        "##setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R4UHfjqkRyg"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu datasets evaluate nltk pandas tqdm\n",
        "\n",
        "import os, re, time, math, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import faiss\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnyeawomkXDL"
      },
      "outputs": [],
      "source": [
        "#Load dataset (SciFact-style: question + answer + optional evidence)\n",
        "# This uses a lightweight public dataset via datasets. If you want your own dataset later, you’ll just supply the same schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVBEWCWMkeg7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Small, fast demo dataset\n",
        "# We'll use \"scifact\" (question/claim + abstracts) style using the BEIR variant isn't directly in HF in one format,\n",
        "# so we will use a simpler QA dataset for correctness metrics and treat retrieved docs as citations.\n",
        "# Recommended for \"evidence doc id\" experiments: later swap to BEIR SciFact with explicit corpus/qrels.\n",
        "\n",
        "ds = load_dataset(\"nq_open\", split=\"train[:200]\")  # keep small for Colab\n",
        "ds = ds.map(lambda x: {\"question\": x[\"question\"], \"answer\": x[\"answer\"][0] if len(x[\"answer\"]) else \"\"})\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMqYtY7kxM1"
      },
      "source": [
        "Cell 3 — Build a tiny document corpus\n",
        "\n",
        "For controlled experiments, you need a fixed corpus. For demo, we’ll build a corpus from Wikipedia-like snippets shipped in HF datasets (fast).\n",
        "Later, you can replace this with your paper PDFs, website dumps, or your own dataset corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zBVHrv3bkrJZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Modern Wikipedia dataset (no script error)\n",
        "corpus_ds = load_dataset(\n",
        "    \"wikimedia/wikipedia\",\n",
        "    \"20231101.en\",\n",
        "    split=\"train[:2000]\"\n",
        ")\n",
        "\n",
        "raw_docs = [\n",
        "    d[\"text\"].replace(\"\\n\", \" \").strip()\n",
        "    for d in corpus_ds\n",
        "    if d.get(\"text\")\n",
        "]\n",
        "\n",
        "raw_docs = [t for t in raw_docs if len(t) > 400]\n",
        "raw_docs = raw_docs[:1500]\n",
        "\n",
        "print(len(raw_docs))\n",
        "print(raw_docs[0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWgcoLTzk3Ty"
      },
      "source": [
        "Cell 4 — Chunking functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6v9b5R7PlARV"
      },
      "outputs": [],
      "source": [
        "def chunk_fixed(text, chunk_size=512, overlap=64):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "        i += max(1, chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "def chunk_sentence(text, max_words=220):\n",
        "    sents = sent_tokenize(text)\n",
        "    chunks, buf = [], []\n",
        "    buf_words = 0\n",
        "    for s in sents:\n",
        "        w = s.split()\n",
        "        if buf_words + len(w) > max_words and buf:\n",
        "            chunks.append(\" \".join(buf))\n",
        "            buf, buf_words = [], 0\n",
        "        buf.append(s)\n",
        "        buf_words += len(w)\n",
        "    if buf:\n",
        "        chunks.append(\" \".join(buf))\n",
        "    return chunks\n",
        "\n",
        "def build_chunks(corpus_texts, strategy):\n",
        "    all_chunks = []\n",
        "    for doc_id, text in enumerate(corpus_texts):\n",
        "        if strategy[\"type\"] == \"fixed\":\n",
        "            chunks = chunk_fixed(text, strategy[\"chunk_size\"], strategy[\"overlap\"])\n",
        "        elif strategy[\"type\"] == \"sentence\":\n",
        "            chunks = chunk_sentence(text, strategy[\"max_words\"])\n",
        "        else:\n",
        "            raise ValueError(\"Unknown chunking strategy\")\n",
        "        for ci, c in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": f\"{doc_id}_{ci}\",\n",
        "                \"text\": c\n",
        "            })\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBxN2vzElF_u"
      },
      "source": [
        "Cell 5 — Embeddings + FAISS index builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jhJ5K_e6lDNJ"
      },
      "outputs": [],
      "source": [
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embs = embedder.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embs.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embs)\n",
        "    return index, embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgqP2LelLH0"
      },
      "source": [
        "Cell 5 — Embeddings + FAISS index builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7U7zw8MplJDO"
      },
      "outputs": [],
      "source": [
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embs = embedder.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embs.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embs)\n",
        "    return index, embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3vkPrHSlTIR"
      },
      "source": [
        "Cell 6 — Retriever + optional reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FC4QdDvelYvX"
      },
      "outputs": [],
      "source": [
        "reranker_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(reranker_name)\n",
        "\n",
        "def retrieve(query, index, chunks, top_k=5):\n",
        "    q_emb = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)\n",
        "    scores, ids = index.search(q_emb, top_k)\n",
        "    hits = []\n",
        "    for score, idx in zip(scores[0], ids[0]):\n",
        "        c = chunks[int(idx)]\n",
        "        hits.append({**c, \"score\": float(score)})\n",
        "    return hits\n",
        "\n",
        "def rerank(query, hits, top_k=5):\n",
        "    pairs = [(query, h[\"text\"]) for h in hits]\n",
        "    rr_scores = reranker.predict(pairs)\n",
        "    reranked = []\n",
        "    for h, s in zip(hits, rr_scores):\n",
        "        reranked.append({**h, \"rr_score\": float(s)})\n",
        "    reranked.sort(key=lambda x: x[\"rr_score\"], reverse=True)\n",
        "    return reranked[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a25CiP3ulgvw"
      },
      "source": [
        "Cell 7 — Load a small instruct LLM (4-bit)\n",
        "\n",
        "This runs on typical Colab T4/L4. If you get OOM, reduce model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KvKvh9lklcPd"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ddTg6WlknI"
      },
      "source": [
        "Cell 8 — Prompts + generation wrapper (with citation format)\n",
        "\n",
        "We’ll force citations like [1], [2] pointing to retrieved chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "amX1gVgqlqsU"
      },
      "outputs": [],
      "source": [
        "def build_prompt(question, contexts, tactic=\"normal\"):\n",
        "    # contexts: list of dicts with \"text\"\n",
        "    # Create numbered sources\n",
        "    sources = []\n",
        "    for i, c in enumerate(contexts, start=1):\n",
        "        sources.append(f\"[{i}] {c['text']}\")\n",
        "    sources_block = \"\\n\\n\".join(sources)\n",
        "\n",
        "    if tactic == \"normal\":\n",
        "        instr = (\n",
        "            \"Answer the question using the sources below. \"\n",
        "            \"If unsure, say you don't know. Add citations like [1] after the sentences they support.\"\n",
        "        )\n",
        "    elif tactic == \"strict\":\n",
        "        instr = (\n",
        "            \"You MUST answer using ONLY the sources below. \"\n",
        "            \"If the answer is not explicitly stated, reply exactly: \\\"I don't know based on the provided sources.\\\" \"\n",
        "            \"Every factual sentence must end with at least one citation like [1].\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unknown tactic\")\n",
        "\n",
        "    user = f\"Question: {question}\\n\\nSources:\\n{sources_block}\\n\\n{instr}\\nAnswer:\"\n",
        "    return user\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_answer(prompt, decoding):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=decoding[\"max_new_tokens\"],\n",
        "        do_sample=decoding[\"temperature\"] > 0,\n",
        "        temperature=decoding[\"temperature\"],\n",
        "        top_p=decoding[\"top_p\"],\n",
        "        repetition_penalty=1.05,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    latency = time.perf_counter() - t0\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # Keep only after \"Answer:\" (best-effort)\n",
        "    ans = text.split(\"Answer:\")[-1].strip()\n",
        "    prompt_tokens = int(inputs[\"input_ids\"].shape[1])\n",
        "    gen_tokens = int(out.shape[1] - inputs[\"input_ids\"].shape[1])\n",
        "    return ans, latency, prompt_tokens, gen_tokens\n",
        "\n",
        "def extract_citations(answer_text):\n",
        "    # returns cited source indices like [1], [2]\n",
        "    cites = re.findall(r\"\\[(\\d+)\\]\", answer_text)\n",
        "    return [int(c) for c in cites if c.isdigit()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZIIx7UcltzL"
      },
      "source": [
        "Cell 9 — Metrics (quality + citation accuracy + basic faithfulness proxy)\n",
        "\n",
        "For demo:\n",
        "\n",
        "Quality: token-level F1 (SQuAD-like)\n",
        "\n",
        "Citation accuracy: proportion of cited ids that are valid, plus “has any citation”\n",
        "\n",
        "Faithfulness proxy (cheap): answer sentences that have at least one citation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4hVkQhDhl0EH"
      },
      "outputs": [],
      "source": [
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "def squad_like_f1(pred, gold):\n",
        "    # evaluate.squad expects list of dicts\n",
        "    refs = [{\"id\":\"0\", \"answers\":{\"text\":[gold], \"answer_start\":[0]}}]\n",
        "    preds = [{\"id\":\"0\", \"prediction_text\": pred}]\n",
        "    return squad_metric.compute(predictions=preds, references=refs)[\"f1\"]\n",
        "\n",
        "def faithfulness_proxy(answer):\n",
        "    sents = [s.strip() for s in re.split(r\"[.\\n]\", answer) if s.strip()]\n",
        "    if not sents:\n",
        "        return 0.0\n",
        "    cited = 0\n",
        "    for s in sents:\n",
        "        if re.search(r\"\\[\\d+\\]\", s):\n",
        "            cited += 1\n",
        "    return cited / len(sents)\n",
        "\n",
        "def citation_metrics(answer, n_sources):\n",
        "    cites = extract_citations(answer)\n",
        "    if len(cites) == 0:\n",
        "        return {\"has_citation\": 0, \"valid_cite_rate\": 0.0}\n",
        "    valid = sum(1 for c in cites if 1 <= c <= n_sources)\n",
        "    return {\n",
        "        \"has_citation\": 1,\n",
        "        \"valid_cite_rate\": valid / len(cites)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ou7WSll8dv"
      },
      "source": [
        "Cell 10 — Experiment grid + runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_wvj200l2wv"
      },
      "outputs": [],
      "source": [
        "chunking_grid = [\n",
        "    {\"name\":\"fixed_512\", \"type\":\"fixed\", \"chunk_size\":512, \"overlap\":64},\n",
        "    {\"name\":\"fixed_256\", \"type\":\"fixed\", \"chunk_size\":256, \"overlap\":32},\n",
        "    {\"name\":\"sentence_220w\", \"type\":\"sentence\", \"max_words\":220},\n",
        "]\n",
        "\n",
        "retrieval_grid = [\n",
        "    {\"top_k\": 3},\n",
        "    {\"top_k\": 5},\n",
        "    {\"top_k\": 10},\n",
        "]\n",
        "\n",
        "rerank_grid = [\n",
        "    {\"rerank\": False},\n",
        "    {\"rerank\": True},\n",
        "]\n",
        "\n",
        "prompt_grid = [\n",
        "    {\"tactic\": \"normal\"},\n",
        "    {\"tactic\": \"strict\"},\n",
        "]\n",
        "\n",
        "decoding_grid = [\n",
        "    {\"name\":\"deterministic\", \"temperature\":0.0, \"top_p\":1.0, \"max_new_tokens\":160},\n",
        "    {\"name\":\"balanced\", \"temperature\":0.4, \"top_p\":0.9, \"max_new_tokens\":160},\n",
        "]\n",
        "\n",
        "def run_experiment(ds, corpus_texts, n_questions=50):\n",
        "    rows = []\n",
        "    sample = ds.select(range(min(n_questions, len(ds))))\n",
        "\n",
        "    for ch in chunking_grid:\n",
        "        print(f\"\\n--- Building chunks & index: {ch['name']} ---\")\n",
        "        chunks = build_chunks(corpus_texts, ch)\n",
        "        index, _ = build_faiss_index(chunks)\n",
        "\n",
        "        for r in retrieval_grid:\n",
        "            for rr in rerank_grid:\n",
        "                for pg in prompt_grid:\n",
        "                    for dec in decoding_grid:\n",
        "                        config_id = f\"{ch['name']}|k{r['top_k']}|rr{int(rr['rerank'])}|{pg['tactic']}|{dec['name']}\"\n",
        "                        print(f\"\\nRunning: {config_id}\")\n",
        "\n",
        "                        for ex in tqdm(sample, desc=config_id):\n",
        "                            q = ex[\"question\"]\n",
        "                            gold = ex[\"answer\"]\n",
        "\n",
        "                            t_retr0 = time.perf_counter()\n",
        "                            hits = retrieve(q, index, chunks, top_k=r[\"top_k\"])\n",
        "                            retr_latency = time.perf_counter() - t_retr0\n",
        "\n",
        "                            if rr[\"rerank\"]:\n",
        "                                t_rr0 = time.perf_counter()\n",
        "                                hits = rerank(q, hits, top_k=r[\"top_k\"])\n",
        "                                rr_latency = time.perf_counter() - t_rr0\n",
        "                            else:\n",
        "                                rr_latency = 0.0\n",
        "\n",
        "                            prompt = build_prompt(q, hits, tactic=pg[\"tactic\"])\n",
        "                            pred, gen_latency, p_tok, g_tok = generate_answer(prompt, dec)\n",
        "\n",
        "                            f1 = squad_like_f1(pred, gold)\n",
        "                            faith = faithfulness_proxy(pred)\n",
        "                            cmet = citation_metrics(pred, n_sources=len(hits))\n",
        "\n",
        "                            rows.append({\n",
        "                                \"config\": config_id,\n",
        "                                \"question\": q,\n",
        "                                \"gold\": gold,\n",
        "                                \"pred\": pred,\n",
        "                                \"f1\": f1,\n",
        "                                \"faithfulness_proxy\": faith,\n",
        "                                \"has_citation\": cmet[\"has_citation\"],\n",
        "                                \"valid_cite_rate\": cmet[\"valid_cite_rate\"],\n",
        "                                \"retrieval_latency_s\": retr_latency,\n",
        "                                \"rerank_latency_s\": rr_latency,\n",
        "                                \"gen_latency_s\": gen_latency,\n",
        "                                \"prompt_tokens\": p_tok,\n",
        "                                \"gen_tokens\": g_tok,\n",
        "                                \"total_latency_s\": retr_latency + rr_latency + gen_latency\n",
        "                            })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df = run_experiment(ds, raw_docs, n_questions=30)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHg1vWZGmDBh"
      },
      "source": [
        "Cell 11 — Aggregate results + save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmpFdF1RmILC"
      },
      "outputs": [],
      "source": [
        "agg = (df.groupby(\"config\")\n",
        "       .agg(\n",
        "           mean_f1=(\"f1\",\"mean\"),\n",
        "           mean_faith=(\"faithfulness_proxy\",\"mean\"),\n",
        "           citation_presence=(\"has_citation\",\"mean\"),\n",
        "           mean_valid_cite=(\"valid_cite_rate\",\"mean\"),\n",
        "           mean_total_latency=(\"total_latency_s\",\"mean\"),\n",
        "           mean_prompt_tokens=(\"prompt_tokens\",\"mean\"),\n",
        "           mean_gen_tokens=(\"gen_tokens\",\"mean\"),\n",
        "       )\n",
        "       .reset_index()\n",
        "       .sort_values([\"mean_f1\",\"mean_faith\",\"mean_valid_cite\",\"mean_total_latency\"], ascending=[False,False,False,True]))\n",
        "\n",
        "display(agg.head(20))\n",
        "\n",
        "df.to_csv(\"rag_design_study_runs.csv\", index=False)\n",
        "agg.to_csv(\"rag_design_study_summary.csv\", index=False)\n",
        "\n",
        "print(\"Saved: rag_design_study_runs.csv, rag_design_study_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yar6RlocmZQg"
      },
      "source": [
        "2) What you do after running this code\n",
        "\n",
        "Start with the demo (as-is) and confirm it runs end-to-end.\n",
        "\n",
        "You’ll get rag_design_study_runs.csv (per-question) and rag_design_study_summary.csv (per-config).\n",
        "\n",
        "Switch from the demo corpus to your real corpus (papers/articles).\n",
        "\n",
        "Replace raw_docs with your documents:\n",
        "\n",
        "If you have PDFs: extract text (PyMuPDF) → list of strings.\n",
        "\n",
        "If you have web pages: store cleaned text per page.\n",
        "\n",
        "Switch from the demo QA dataset to your evaluation datasets.\n",
        "\n",
        "Make a CSV with columns: question, answer (and optionally gold_doc_id).\n",
        "\n",
        "Load it in Colab and convert to the same schema used in the notebook.\n",
        "\n",
        "Add interaction analyses (your “pairwise interaction effects” objective).\n",
        "\n",
        "Keep the grid small (e.g., chunking × top_k) and plot heatmaps:\n",
        "\n",
        "mean_f1 vs (chunking, top_k)\n",
        "\n",
        "mean_latency vs (rerank, top_k)\n",
        "\n",
        "Upgrade “faithfulness/factuality” measurement for your paper.\n",
        "\n",
        "Keep the cheap proxy in the tables (it’s useful), but add:\n",
        "\n",
        "an LLM-as-a-judge faithfulness score (open-source judge or API),\n",
        "\n",
        "and a citation-to-evidence alignment check (sentence-level entailment, or judge)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R08uazJXox2q"
      },
      "source": [
        "For a proper RAG paper, you should eventually move to:\n",
        "\n",
        "BEIR SciFact\n",
        "\n",
        "BEIR Natural Questions\n",
        "\n",
        "HotpotQA\n",
        "\n",
        "your own domain corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erQCN802mTIr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMqV0lfnKIc2gC/6RLFm6xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}